{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69797a1-29c3-4a75-86d2-9ca1b1c6acf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c352f-b14a-42a7-8961-15e8ee9decce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12463f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059757b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile, BadZipFile\n",
    "import os\n",
    "def getDataExtracted():\n",
    "    def extract_zip_file(extract_path):\n",
    "        try:\n",
    "            with ZipFile(extract_path+\".zip\") as zfile:\n",
    "                zfile.extractall(extract_path)\n",
    "            # remove zipfile\n",
    "            zfileTOremove=f\"{extract_path}\"+\".zip\"\n",
    "            if os.path.isfile(zfileTOremove):\n",
    "                os.remove(zfileTOremove)\n",
    "            else:\n",
    "                print(\"Error: %s file not found\" % zfileTOremove)    \n",
    "        except BadZipFile as e:\n",
    "            print(\"Error:\", e)\n",
    "    extract_train_path = \"./coco_train2017\"\n",
    "    extract_val_path = \"./coco_val2017\"\n",
    "    extract_ann_path=\"./coco_ann2017\"\n",
    "\n",
    "    extract_zip_file(extract_ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataExtracted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'coco_ann2017'\n",
    "dataType = 'train2017'\n",
    "annFile = os.path.join(dataDir, 'annotations', f'instances_{dataType}.json')\n",
    "coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f72813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataType = 'train2017'  \n",
    "dataDir = 'coco_train2017'\n",
    "imgIds = coco.getImgIds()\n",
    "\n",
    "chosen_img_id = imgIds[1000]\n",
    "img_info = coco.loadImgs(chosen_img_id)[0]\n",
    "img_path = os.path.join(dataDir, dataType, img_info['file_name'])\n",
    "print(img_path)\n",
    "if os.path.exists(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    width, height = image.size\n",
    "    annIds = coco.getAnnIds(imgIds=chosen_img_id)\n",
    "    annotations = coco.loadAnns(annIds)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']\n",
    "        x, y, w, h = bbox\n",
    "        x_min = x\n",
    "        y_min = y\n",
    "        x_max = x + w\n",
    "        y_max = y + h\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: Image file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxy1 = 0\n",
    "maxy2 = 0\n",
    "for i in range (len(imgIds)):\n",
    "    image = Image.open(img_path)\n",
    "    maxy = max(maxy, image.size[1])\n",
    "    maxy2 = max(maxy2, image.size[0])\n",
    "print(maxy)\n",
    "print(maxy2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a757d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06656206",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = 'resize_train'\n",
    "os.makedirs(outputDir, exist_ok=True)\n",
    "target_width = 640\n",
    "target_height = 457\n",
    "new_annotations = []\n",
    "dataType = 'train2017'  \n",
    "dataDir = 'coco_train2017'\n",
    "imgIds = coco.getImgIds()\n",
    "\n",
    "for i in range(1000):\n",
    "    imgId = imgIds[i]\n",
    "    chosen_img_id = imgIds[i]\n",
    "    img_info = coco.loadImgs(chosen_img_id)[0]\n",
    "    img_path = os.path.join(dataDir, dataType, img_info['file_name'])\n",
    "    image = Image.open(img_path)\n",
    "    img_filename = img_info['file_name']\n",
    "    image.thumbnail((target_width, target_height), Image.ANTIALIAS)\n",
    "    pad_width = target_width - image.width\n",
    "    pad_height = target_height - image.height\n",
    "    padding = (0, 0, pad_width, pad_height) \n",
    "    image = ImageOps.expand(image, padding)\n",
    "    \n",
    "    resized_img_path = os.path.join(outputDir, img_filename)\n",
    "    image.save(resized_img_path)\n",
    "    \n",
    "    new_annotation = {\n",
    "        'image_id': chosen_img_id,\n",
    "        'file_name': img_filename,\n",
    "        'width': target_width,\n",
    "        'height': target_height,\n",
    "        'annotations': []  \n",
    "    }\n",
    "    \n",
    "    annIds = coco.getAnnIds(imgIds=chosen_img_id)\n",
    "    annotations = coco.loadAnns(annIds)\n",
    "    \n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']\n",
    "        x, y, w, h = bbox\n",
    "        x_ratio = target_width / image.width\n",
    "        y_ratio = target_height / image.height\n",
    "        new_x = round(x * x_ratio)\n",
    "        new_y = round(y * y_ratio)\n",
    "        new_w = round(w * x_ratio)\n",
    "        new_h = round(h * y_ratio)\n",
    "        new_bbox = [new_x, new_y, new_w, new_h]\n",
    "        new_annotation['annotations'].append(new_bbox)\n",
    "    \n",
    "    new_annotations.append(new_annotation)\n",
    "\n",
    "new_annFile =  'instances_resized.json'\n",
    "with open(new_annFile, 'w') as f:\n",
    "    json.dump(new_annotations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d1b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_width = 640\n",
    "target_height = 457\n",
    "counter= 0\n",
    "folder_path = \"resize_train\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            counter+=1\n",
    "            if width != target_width or height != target_height:\n",
    "                print(f\"Image {filename} has size {width}x{height}.\")\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ff9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = 'resize_train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f646b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if bounding boxs work with the new images    \n",
    "new_annFile = 'instances_resized.json'\n",
    "with open(new_annFile, 'r') as f:\n",
    "    new_annotations = json.load(f)\n",
    "image_data = new_annotations[100]\n",
    "image_path = os.path.join(outputDir, image_data['file_name'])\n",
    "annotations = image_data['annotations']\n",
    "image = Image.open(image_path)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "for bbox in annotations:\n",
    "    x, y, w, h = bbox\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_resized_file = 'instances_resized.json'\n",
    "with open(instances_resized_file, 'r') as f:\n",
    "    instances_resized_data = json.load(f)\n",
    "annotations_dict = {entry['file_name']: entry['annotations'] for entry in instances_resized_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ed1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_width = 640\n",
    "target_height = 457\n",
    "def getBinaryMask(annotations,target_height, target_width):\n",
    "    mask = np.zeros((target_height + 1, target_width), dtype=np.uint8)\n",
    "    for bbox in annotations:\n",
    "        x, y, w, h = bbox\n",
    "        mask[y:y+h, x:x+w] = 1\n",
    "    mask[-1] = len(annotations)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing binary mask \n",
    "a = [[1,1,2,2]]\n",
    "mask = getBinaryMask(a, 5,5)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"resize_train\"\n",
    "image_filenames = [filename for filename in os.listdir(folder_path) if filename.endswith((\".jpg\", \".png\"))]\n",
    "image_data_list = []\n",
    "target_height = 457\n",
    "target_width = 640\n",
    "annotations_list = []\n",
    "for filename in image_filenames:\n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    r, g, b = image.split()\n",
    "    r_array = np.array(r)\n",
    "    g_array = np.array(g)\n",
    "    b_array = np.array(b)\n",
    "    rgb_array = np.stack((r_array, g_array, b_array), axis=-1)\n",
    "    np_arr = np.zeros((rgb_array.shape[0] + 1, rgb_array.shape[1], 3))\n",
    "    np_arr[:rgb_array.shape[0],:,:] = rgb_array\n",
    "    np_arr[-1,:,:] = 0\n",
    "    image_data_list.append(np_arr)\n",
    "    annotations = annotations_dict[filename]\n",
    "    binary_mask = getBinaryMask(annotations, target_height, target_width)\n",
    "    annotations_list.append(binary_mask)\n",
    "X_train = np.array(image_data_list)\n",
    "X_train  = np.transpose(X_train, (0, 3, 1, 2))\n",
    "Y_train = np.array(annotations_list)\n",
    "print(\"Shape of Training Data array:\", X_train.shape)\n",
    "print(\"Shape of Output array:\", Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, learning_rate = 1e-5, optimizer = \"adam\"):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "        nn.Conv2d(3,4,kernel_size =(5, 5),padding = 'same'),\n",
    "        # nn.ReLU(),\n",
    "        # nn.Conv2d(32,16,kernel_size =(32, 32),padding = 'same'),\n",
    "        nn.BatchNorm2d(4, eps=1e-05, momentum=0.1), \n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(4,8,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.BatchNorm2d(8, eps=1e-05, momentum=0.1), \n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(8,4,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(4,2,kernel_size =(5,5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(2,2,kernel_size =(5,5),padding = 'same'),\n",
    "        nn.BatchNorm2d(2, eps=1e-05, momentum=0.1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(2,1,kernel_size =(5,5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(1,1,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU()\n",
    "        ]\n",
    "        self.layers = layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay = 1e-3)\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out.squeeze() \n",
    "    def loss_func(self, x, target):\n",
    "        res = self.forward(x)\n",
    "        return torch.mean(torch.square(res - target))\n",
    "    def backprop(self, x, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_func(x, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f18a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Detector2(nn.Module):\n",
    "    def __init__(self, learning_rate = 1e-5, optimizer = \"adam\"):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "        nn.Conv2d(3,64,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64,32,kernel_size =(32, 32),padding = 'same'),\n",
    "        nn.BatchNorm2d(32, eps=1e-05, momentum=0.1), \n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(32,16,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.BatchNorm2d(16, eps=1e-05, momentum=0.1), \n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(16,8,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(8,4,kernel_size =(5,5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(4,2,kernel_size =(5,5),padding = 'same'),\n",
    "        nn.BatchNorm2d(2, eps=1e-05, momentum=0.1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(2,1,kernel_size =(5,5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(1,1,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU()\n",
    "        ]\n",
    "        self.layers = layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay = 1e-3)\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out.squeeze() \n",
    "    def loss_func(self, x, target):\n",
    "        res = self.forward(x)\n",
    "        return torch.mean(torch.square(res - target))\n",
    "    def backprop(self, x, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_func(x, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f57ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Detector3(nn.Module):\n",
    "    def __init__(self, learning_rate = 1e-5, optimizer = \"adam\"):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "        nn.Conv2d(3,16,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16,16,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = (2,2), stride = 2, padding='none'),\n",
    "        nn.Conv2d(8,8,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = (2,2), stride = 2, padding='none'),\n",
    "        nn.Conv2d(4,4,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = (2,2), stride = 2, padding='none'),\n",
    "        nn.Conv2d(2,2,kernel_size =(5, 5),padding = 'same'),\n",
    "        nn.\n",
    "        torch.nn.Flatten()\n",
    "        torch.nn.Linear(in_features, out_features),\n",
    "        ]\n",
    "        self.layers = layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay = 1e-3)\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out.squeeze() \n",
    "    def loss_func(self, x, target):\n",
    "        res = self.forward(x)\n",
    "        return torch.mean(torch.square(res - target))\n",
    "    def backprop(self, x, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_func(x, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detector()\n",
    "model.train()\n",
    "n_epochs = 10\n",
    "BATCH_SIZE = 32\n",
    "index_train = np.array([i for i in range(1000)])\n",
    "losses = []\n",
    "#SUB Test for Train\n",
    "for epoch in range(n_epochs):\n",
    "    new_index = np.random.choice(index_train, BATCH_SIZE)\n",
    "    new_X_train = X_train[new_index]\n",
    "    new_Y_train = Y_train[new_index]\n",
    "    X_tensor = torch.tensor(new_X_train, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(new_Y_train, dtype=torch.float32)       \n",
    "    model.optimizer.zero_grad()\n",
    "    Y_pred = model(X_tensor)\n",
    "    loss = model.loss_func(X_tensor, Y_tensor)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_epochs+1), losses, label='Training Loss')\n",
    "plt.title('Epoch vs. Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56095ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = np.random.rand(2000, 10)  \n",
    "Y_test = np.random.rand(2000, 1)   \n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "    Y_pred_test = model(X_test_tensor)\n",
    "    test_loss = model.loss_func(Y_pred_test, Y_test_tensor).item()\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = Detector()\n",
    "values = model(torch.from_numpy(X_test[:2]).float()).numpy()\n",
    "numpy_array = np.random.rand(458, 640)\n",
    "last_row_avg = np.round(np.mean(numpy_array[-1]))\n",
    "num_centers = int(last_row_avg)\n",
    "data = numpy_array[:-1, :]\n",
    "data_1d = data.reshape(-1, 1)\n",
    "kmeans = KMeans(n_clusters=num_centers, random_state=0)\n",
    "kmeans.fit(data_1d)\n",
    "labels = kmeans.labels_\n",
    "labels_2d = labels.reshape(data.shape)\n",
    "n = 5\n",
    "image_path = f'resize_train/{n}.jpg'\n",
    "image = Image.open(image_path)\n",
    "image_array = np.array(image)\n",
    "\n",
    "bounding_boxes = []\n",
    "for label in np.unique(labels):\n",
    "    indices = np.argwhere(labels_2d == label)\n",
    "    if len(indices) > 0:\n",
    "        min_y, min_x = np.min(indices, axis=0)\n",
    "        max_y, max_x = np.max(indices, axis=0)\n",
    "        bounding_boxes.append([(min_x, min_y), (max_x, max_y)])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image_array)\n",
    "for bbox in bounding_boxes:\n",
    "    min_pt, max_pt = bbox\n",
    "    rect = plt.Rectangle(min_pt, max_pt[0] - min_pt[0], max_pt[1] - min_pt[1],\n",
    "                         linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c0e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b60717af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "619626ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33cb69c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96ce7cef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe9b3a2",
   "metadata": {},
   "source": [
    "THIS IS THE BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "def load_and_convert_image(image_path):\n",
    "    image = io.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def display_image(image, axis='off'):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(axis)\n",
    "    plt.show()\n",
    "\n",
    "def perform_kmeans_clustering(image, k=4):\n",
    "    pixel_values = image.reshape((-1, 3))\n",
    "    pixel_values = np.float32(pixel_values)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return labels, centers\n",
    "\n",
    "def create_segmented_image(image, labels, centers):\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_image = segmented_data.reshape(image.shape)\n",
    "    return segmented_image\n",
    "\n",
    "def mask_cluster(image, labels, cluster=2, color=(0,0,255)):\n",
    "    labels_reshaped = labels.reshape(image.shape[0], image.shape[1])\n",
    "    masked_image = np.copy(image)\n",
    "    masked_image[labels_reshaped == cluster] = [color]\n",
    "    return masked_image\n",
    "\n",
    "def find_and_draw_largest_contour(image, contour_color=(255,0,0), pad=3):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    blue = np.uint8([[[255,0,0]]])\n",
    "    hsv_blue = cv2.cvtColor(blue, cv2.COLOR_BGR2HSV)\n",
    "    lower_blue, upper_blue = (120,255,250), (120,255,255)\n",
    "    threshed_image = cv2.inRange(hsv_image, np.array([lower_blue],np.uint8), np.array([upper_blue],np.uint8))\n",
    "    _, thresh = cv2.threshold(threshed_image,127,255,0)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    max_index = np.argmax(areas)\n",
    "    cnt = contours[max_index]\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    cv2.rectangle(image, (x-pad, y-pad), (x+w+pad, y+h+pad), contour_color, 2)\n",
    "    return image\n",
    "\n",
    "image_path = 'train.jpg'\n",
    "image = load_and_convert_image(image_path)\n",
    "display_image(image)\n",
    "labels, centers = perform_kmeans_clustering(image)\n",
    "segmented_image = create_segmented_image(image, labels, centers)\n",
    "display_image(segmented_image)\n",
    "masked_image = mask_cluster(image, labels)\n",
    "display_image(masked_image)\n",
    "final_image = find_and_draw_largest_contour(masked_image)\n",
    "display_image(final_image, axis='on')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def load_and_convert_image(image_path):\n",
    "    image = io.imread(image_path)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def display_image(image, axis='off'):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(axis)\n",
    "    plt.show()\n",
    "\n",
    "def perform_kmeans_clustering(image, k=3):\n",
    "    pixel_values = image.reshape((-1, 3))\n",
    "    pixel_values = np.float32(pixel_values)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return labels, centers\n",
    "\n",
    "def create_segmented_image(image, labels, centers):\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_image = segmented_data.reshape(image.shape)\n",
    "    return segmented_image\n",
    "\n",
    "def load_pytorch_model():\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image_for_pytorch(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((800, 800)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "def detect_objects_pytorch(model, image):\n",
    "    image_tensor = preprocess_image_for_pytorch(image)\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor])[0]\n",
    "    return prediction\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
    "    'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return COCO_INSTANCE_CATEGORY_NAMES[class_id]\n",
    "\n",
    "def draw_detection_boxes(image, prediction, threshold=0.5):\n",
    "    for element in range(len(prediction['boxes'])):\n",
    "        if prediction['scores'][element] > threshold:\n",
    "            box = prediction['boxes'][element].cpu().numpy()\n",
    "            class_id = prediction['labels'][element].item()-1\n",
    "            class_name = get_class_name(class_id)\n",
    "            print('Detected:', class_name)\n",
    "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "            cv2.putText(image, class_name, (int(box[0]), int(box[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "    return image\n",
    "\n",
    "\n",
    "image_path = 'dog.jpg'  \n",
    "image = load_and_convert_image(image_path)\n",
    "display_image(image)\n",
    "labels, centers = perform_kmeans_clustering(image)\n",
    "segmented_image = create_segmented_image(image, labels, centers)\n",
    "display_image(segmented_image)\n",
    "model = load_pytorch_model()\n",
    "prediction = detect_objects_pytorch(model, image)\n",
    "final_image = draw_detection_boxes(image, prediction)\n",
    "display_image(final_image, axis='on')\n",
    "image_path = 'sheep.jpeg'  \n",
    "image = load_and_convert_image(image_path)\n",
    "display_image(image)\n",
    "labels, centers = perform_kmeans_clustering(image)\n",
    "segmented_image = create_segmented_image(image, labels, centers)\n",
    "display_image(segmented_image)\n",
    "model = load_pytorch_model()\n",
    "prediction = detect_objects_pytorch(model, image)\n",
    "final_image = draw_detection_boxes(image, prediction)\n",
    "display_image(final_image, axis='on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab42ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578afe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "dataDir = 'coco_train2017'\n",
    "outputDir = 'resize_train'\n",
    "os.makedirs(outputDir, exist_ok=True)\n",
    "target_width = 64\n",
    "target_height = 46\n",
    "\n",
    "new_annFile = os.path.join(outputDir, f'instances_{dataType}_resized.json')\n",
    "new_annotations = []\n",
    "\n",
    "for img_id in coco.getImgIds():\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = os.path.join(dataDir, dataType, img_info['file_name'])\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    width, height = image.size\n",
    "    if width > target_width or height > target_height:\n",
    "        left = (width - target_width) // 2\n",
    "        top = (height - target_height) // 2\n",
    "        right = left + target_width\n",
    "        bottom = top + target_height\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "    elif width < target_width or height < target_height:\n",
    "        pad_width = max(0, (target_width - width) // 2)\n",
    "        pad_height = max(0, (target_height - height) // 2)\n",
    "        padding = (pad_width, pad_height, pad_width, pad_height)\n",
    "        image = ImageOps.expand(image, padding)\n",
    "    image = image.resize((target_width, target_height), Image.ANTIALIAS)\n",
    "    annIds = coco.getAnnIds(imgIds=img_id)\n",
    "    annotations = coco.loadAnns(annIds)\n",
    "    new_annotations.extend(annotations)\n",
    "    for ann in new_annotations[-len(annotations):]:\n",
    "        bbox = ann['bbox']\n",
    "        x, y, w, h = bbox\n",
    "        ann['bbox'] = [x + pad_width, y + pad_height, w, h]\n",
    "    resized_img_path = os.path.join(outputDir, img_info['file_name'])\n",
    "    image.save(resized_img_path)\n",
    "\n",
    "with open(new_annFile, 'w') as f:\n",
    "    json.dump(new_annotations, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c3a38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1e6126b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b90e4ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b465073",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c03597a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "def load_and_convert_image(image_path):\n",
    "    image = io.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def display_image(image, axis='off'):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(axis)\n",
    "    plt.show()\n",
    "\n",
    "def perform_kmeans_clustering(image, k=4):\n",
    "    pixel_values = image.reshape((-1, 3))\n",
    "    pixel_values = np.float32(pixel_values)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return labels, centers\n",
    "\n",
    "def create_segmented_image(image, labels, centers):\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_image = segmented_data.reshape(image.shape)\n",
    "    return segmented_image\n",
    "\n",
    "def mask_cluster(image, labels, cluster=2, color=(0,0,255)):\n",
    "    labels_reshaped = labels.reshape(image.shape[0], image.shape[1])\n",
    "    masked_image = np.copy(image)\n",
    "    masked_image[labels_reshaped == cluster] = [color]\n",
    "    return masked_image\n",
    "\n",
    "def find_and_draw_largest_contour(image, contour_color=(255,0,0), pad=3):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    blue = np.uint8([[[255,0,0]]])\n",
    "    hsv_blue = cv2.cvtColor(blue, cv2.COLOR_BGR2HSV)\n",
    "    lower_blue, upper_blue = (120,255,250), (120,255,255)\n",
    "    threshed_image = cv2.inRange(hsv_image, np.array([lower_blue],np.uint8), np.array([upper_blue],np.uint8))\n",
    "    _, thresh = cv2.threshold(threshed_image,127,255,0)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    max_index = np.argmax(areas)\n",
    "    cnt = contours[max_index]\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    cv2.rectangle(image, (x-pad, y-pad), (x+w+pad, y+h+pad), contour_color, 2)\n",
    "    return image\n",
    "\n",
    "image_path = 'train.jpg'\n",
    "image = load_and_convert_image(image_path)\n",
    "display_image(image)\n",
    "labels, centers = perform_kmeans_clustering(image)\n",
    "segmented_image = create_segmented_image(image, labels, centers)\n",
    "display_image(segmented_image)\n",
    "masked_image = mask_cluster(image, labels)\n",
    "display_image(masked_image)\n",
    "final_image = find_and_draw_largest_contour(masked_image)\n",
    "display_image(final_image, axis='on')  # Display the final image with axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94879cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def load_and_convert_image(image_path):\n",
    "    image = io.imread(image_path)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def display_image(image, axis='off'):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(axis)\n",
    "    plt.show()\n",
    "\n",
    "def perform_kmeans_clustering(image, k=3):\n",
    "    pixel_values = image.reshape((-1, 3))\n",
    "    pixel_values = np.float32(pixel_values)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return labels, centers\n",
    "\n",
    "def create_segmented_image(image, labels, centers):\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_image = segmented_data.reshape(image.shape)\n",
    "    return segmented_image\n",
    "\n",
    "def load_pytorch_model():\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image_for_pytorch(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((800, 800)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "def detect_objects_pytorch(model, image):\n",
    "    image_tensor = preprocess_image_for_pytorch(image)\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor])[0]\n",
    "    return prediction\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
    "    'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return COCO_INSTANCE_CATEGORY_NAMES[class_id]\n",
    "\n",
    "def draw_detection_boxes(image, prediction, threshold=0.5):\n",
    "    for element in range(len(prediction['boxes'])):\n",
    "        if prediction['scores'][element] > threshold:\n",
    "            box = prediction['boxes'][element].cpu().numpy()\n",
    "            class_id = prediction['labels'][element].item()-1\n",
    "            class_name = get_class_name(class_id)\n",
    "            print('Detected:', class_name)\n",
    "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "            cv2.putText(image, class_name, (int(box[0]), int(box[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "    return image\n",
    "\n",
    "\n",
    "image_path = 'dog.jpg'  \n",
    "image = load_and_convert_image(image_path)\n",
    "display_image(image)\n",
    "labels, centers = perform_kmeans_clustering(image)\n",
    "segmented_image = create_segmented_image(image, labels, centers)\n",
    "display_image(segmented_image)\n",
    "model = load_pytorch_model()\n",
    "prediction = detect_objects_pytorch(model, image)\n",
    "final_image = draw_detection_boxes(image, prediction)\n",
    "display_image(final_image, axis='on')\n",
    "image_path = 'sheep.jpeg'  \n",
    "image = load_and_convert_image(image_path)\n",
    "display_image(image)\n",
    "labels, centers = perform_kmeans_clustering(image)\n",
    "segmented_image = create_segmented_image(image, labels, centers)\n",
    "display_image(segmented_image)\n",
    "model = load_pytorch_model()\n",
    "prediction = detect_objects_pytorch(model, image)\n",
    "final_image = draw_detection_boxes(image, prediction)\n",
    "display_image(final_image, axis='on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb5557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
